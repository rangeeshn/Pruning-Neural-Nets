{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T22:12:14.401825Z",
     "start_time": "2019-04-01T22:12:14.389554Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import h5py\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "sns.set()\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(1867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T22:12:39.812295Z",
     "start_time": "2019-04-01T22:12:38.877071Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T22:12:45.500292Z",
     "start_time": "2019-04-01T22:12:45.315618Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1), padding=\"same\"),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "    tf.keras.layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T22:12:47.136065Z",
     "start_time": "2019-04-01T22:12:47.126819Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, dataset):\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "    for x, y in dataset:\n",
    "        outputs = model(x)\n",
    "        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "    return epoch_accuracy.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T22:28:50.266259Z",
     "start_time": "2019-04-01T22:22:09.786198Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 36s 599us/sample - loss: 0.2704 - acc: 0.9200 - val_loss: 0.0738 - val_acc: 0.9777\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 38s 639us/sample - loss: 0.0638 - acc: 0.9800 - val_loss: 0.0523 - val_acc: 0.9819\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 42s 692us/sample - loss: 0.0413 - acc: 0.9872 - val_loss: 0.0441 - val_acc: 0.9860\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 41s 675us/sample - loss: 0.0319 - acc: 0.9897 - val_loss: 0.0431 - val_acc: 0.9866\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 40s 669us/sample - loss: 0.0232 - acc: 0.9927 - val_loss: 0.0507 - val_acc: 0.9850\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 41s 681us/sample - loss: 0.0206 - acc: 0.9932 - val_loss: 0.0381 - val_acc: 0.9892\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 41s 684us/sample - loss: 0.0170 - acc: 0.9948 - val_loss: 0.0404 - val_acc: 0.9891\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 40s 675us/sample - loss: 0.0138 - acc: 0.9955 - val_loss: 0.0352 - val_acc: 0.9892\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 40s 673us/sample - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0334 - val_acc: 0.9902\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 41s 678us/sample - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0308 - val_acc: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor e in range(epochs):\\n    epoch_loss_avg = tf.contrib.eager.metrics.Mean()\\n    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\\n    for x, y in dataset_train:\\n        with tf.GradientTape() as tape:\\n            outputs = model(x)\\n            loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\\n        grads = tape.gradient(loss, model.trainable_weights)\\n        optimizer.apply_gradients(zip(grads, model.trainable_weights), global_step)\\n        epoch_loss_avg(loss)\\n        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\\n    training_losses.append(epoch_loss_avg.result())\\n    training_accuracies.append(epoch_accuracy.result())\\n    \\nmodel.save(\"./LeNet5_results/before_pruning.h5\")\\n\\ntest(model, dataset_test)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\"\"\"\n",
    "for e in range(epochs):\n",
    "    epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "    for x, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(x)\n",
    "            loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights), global_step)\n",
    "        epoch_loss_avg(loss)\n",
    "        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "    training_losses.append(epoch_loss_avg.result())\n",
    "    training_accuracies.append(epoch_accuracy.result())\n",
    "    \n",
    "model.save(\"./LeNet5_results/before_pruning.h5\")\n",
    "\n",
    "test(model, dataset_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T22:30:41.104502Z",
     "start_time": "2019-04-01T22:30:38.357934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.030759480433717182\n",
      "Test accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:32:20.552766Z",
     "start_time": "2019-04-01T20:32:06.387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(1, figsize=(15,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(training_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:41:57.112799Z",
     "start_time": "2019-04-02T02:41:57.088584Z"
    }
   },
   "outputs": [],
   "source": [
    "def prune_weights(dense_model, percentile):\n",
    "    prev_kept_columns = None\n",
    "    pruned_model = tf.keras.models.Sequential()\n",
    "    pruned_model.add(tf.keras.layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1), padding=\"same\"))\n",
    "    pruned_model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "    pruned_model.add(tf.keras.layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    pruned_model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    pruned_model.add(tf.keras.layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    pruned_model.add(tf.keras.layers.Flatten())\n",
    "    pruned_model.add(tf.keras.layers.Dense(84, activation='relu'))\n",
    "\n",
    "    num_layers = len(dense_model.trainable_weights)\n",
    "\n",
    "    print (num_layers)\n",
    "    for i_layer, weights in enumerate(dense_model.trainable_weights):\n",
    "        print (i_layer)\n",
    "        weights_np = weights.numpy()\n",
    "        \n",
    "        # Remove pruned columns\n",
    "        if i_layer < num_layers-1: # Do not prune last layer\n",
    "            column_norms = np.linalg.norm(weights_np, ord=2, axis=0)\n",
    "            critical_value = np.percentile(column_norms, percentile)\n",
    "            keep_mask = column_norms >= critical_value\n",
    "            weights_np = weights_np[:, keep_mask]\n",
    "            \n",
    "        # Remove rows corresponding to previous layer's pruned columns\n",
    "        if prev_kept_columns is not None:\n",
    "            weights_np = weights_np[prev_kept_columns, :]\n",
    "        \n",
    "        # Record which columns were kept\n",
    "        if i_layer < num_layers-1: # No pruned columns in last layer\n",
    "            prev_kept_columns = np.argwhere(keep_mask).reshape(-1)\n",
    "        \n",
    "        # Add new layer to sparse model\n",
    "        if (i_layer == num_layers - 2):\n",
    "            new_layer = tf.keras.layers.Dense(weights_np[0].shape, activation='relu')\n",
    "            pruned_model.add(new_layer)\n",
    "            new_layer.set_weights([weights_np])\n",
    "    \n",
    "    pruned_model.add(tf.keras.layers.Dense(10, activation='softmax')) \n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:42:00.244829Z",
     "start_time": "2019-04-02T02:41:59.928858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-1e9d841bfa55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpruned_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprune_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#pruned_model.save(\"./LeNet300_100_results/after_pruning.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpruned_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-e09e26cbba2b>\u001b[0m in \u001b[0;36mprune_weights\u001b[1;34m(dense_model, percentile)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Add new layer to sparse model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi_layer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mnew_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mpruned_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mnew_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweights_np\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_weights(model, 50)\n",
    "\n",
    "#pruned_model.save(\"./LeNet300_100_results/after_pruning.h5\")\n",
    "score = test(pruned_model, dataset_test)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:32:20.560301Z",
     "start_time": "2019-04-01T20:32:06.398Z"
    }
   },
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "    for x, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = pruned_model(x)\n",
    "            loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "        grads = tape.gradient(loss, pruned_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, pruned_model.trainable_weights), global_step)\n",
    "        epoch_loss_avg(loss)\n",
    "        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "    training_losses.append(epoch_loss_avg.result())\n",
    "    training_accuracies.append(epoch_accuracy.result())\n",
    "    \n",
    "pruned_model.save(\"./LeNet300_100_results/pruned_retrained.h5\")\n",
    "test(pruned_model,dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:32:20.563807Z",
     "start_time": "2019-04-01T20:32:06.404Z"
    }
   },
   "outputs": [],
   "source": [
    "percentiles = [10, 20, 30, 40, 50]\n",
    "\n",
    "pruned_unit_test_losses = []\n",
    "pruned_unit_test_accuracies = []\n",
    "pruned_unit_timings = []\n",
    "\n",
    "for percentile in percentiles:\n",
    "    sparse_model = prune_weights(model, percentile)\n",
    "    t = time.clock()\n",
    "    l, a = test(sparse_model, dataset_test)\n",
    "    t = time.clock() - t\n",
    "    print(f\"Pruning p{percentile} -- Test Loss: {l:.4f}, Test Accuracy: {a:.4f}, Timing: {t:.2f}s\")\n",
    "    pruned_unit_test_losses.append(l)\n",
    "    pruned_unit_test_accuracies.append(a)\n",
    "    pruned_unit_timings.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:32:20.566236Z",
     "start_time": "2019-04-01T20:32:06.409Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(15,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(percentiles, pruned_unit_test_accuracies)\n",
    "plt.xlabel('Sparsity (%)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.subplot(122)\n",
    "plt.plot(percentiles, pruned_unit_test_losses)\n",
    "plt.xlabel('Sparsity (%)')\n",
    "plt.ylabel('Test Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
